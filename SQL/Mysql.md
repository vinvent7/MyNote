# Mysql

ctrl+c 强行停止执行

 history 查看执行过的命令

## 基础操作篇

mysql中SQL语句不区分大小写。

![1](image\mysql\quit.jpg)

![1](image\mysql\sin.jpg)

![1](image\mysql\\c.jpg)

### 创建数据库和表

![1](image\mysql\databases.jpg)

![1](image\mysql\pet.jpg)

### 导入数据

![1](image\mysql\infile.jpg)

### 数据检索

![1](image\mysql\update.jpg)



![1](image\mysql\and.jpg)

![1](image\mysql\括号.jpg)

![1](image\mysql\distinct.jpg)

![1](image\mysql\多列排序.jpg)

![1](image\mysql\日期.jpg)

![1](image\mysql\timestampdiff.jpg)

![1](image\mysql\not null.jpg)

### 最大最小值

1.过滤出某个字段值最大的整条记录数据

使用关联查询时，起个别名，自己关联自己

![1](image\mysql\max.jpg)

![1](image\mysql\min.jpg)

### 聚合函数

![1](image\mysql\count.jpg)

![1](image\mysql\sum.jpg)

### 数据库备份与恢复

![1](image\mysql\数据库备份与恢复.jpg)

### 多表查询

![1](image\mysql\内连接.jpg)

![1](image\mysql\左外连接.jpg)

![1](image\mysql\右外连接.jpg)

![1](image\mysql\子查询.jpg)

![1](image\mysql\exists.jpg)

![1](image\mysql\union.jpg)

#### 关联子查询

![1](image\mysql\case when.jpg)

![1](image\mysql\case when测试.jpg)

![1](image\mysql\group by.jpg)

### 模糊查询

![1](image\mysql\like.jpg)

### 空值处理

![1](image\mysql\null.jpg)

### Mysql元数据

![1](image\mysql\alter.jpg)

![1](image\mysql\modify.jpg)

### MySQL数据类型

#### 1.数值类型

#### 2.日期和时间类型

#### 3.字符串类型



### MySQL函数

#### 字符串函数

![1](image\mysql\字符串函数1.jpg)

![1](image\mysql\字符串函数2.jpg)

![1](image\mysql\字符串函数2.jpg)

![1](image\mysql\字符串函数3.jpg)

![1](image\mysql\字符串函数4.jpg)

#### 数字函数

![1](image\mysql\数字函数.jpg)

#### 日期函数

![1](image\mysql\日期函数.jpg)

#### 高级函数

![1](image\mysql\高级函数.jpg)

### MySQL索引

#### 普通索引

![1](image\mysql\创建索引.jpg)

![1](image\mysql\drop.jpg)

#### 唯一索引

![1](image\mysql\unique.jpg)

#### 添加删除索引

![1](image\mysql\添加删除索引.jpg)

#### 添加删除主键

![1](image\mysql\添加删除主键.jpg)

### MySQL事务

![1](image\mysql\事务.jpg)

#### 事务控制

![1](image\mysql\事务控制.jpg)

#### 事务处理

![1](image\mysql\事务处理.jpg)

### 编码MySQL

![1](image\mysql\查看编码.jpg)

![1](image\mysql\设置编码.jpg)

![1](image\mysql\目录及配置文件.jpg)

![1](image\mysql\日志和端口.jpg)

## 数据库原理

### 数据库系统

![1](image\mysql\模型.jpg)

![1](image\mysql\数据库管理系统.jpg)

![1](image\mysql\数据库系统.jpg)

### 关系代数分类

![1](image\mysql\关系代数分类.jpg)

### 数据库设计

关系型数据库的设计分以下5个阶段

5.1、需求分析

明确用户需求，到底做什么？

![1](image\mysql\概念模式设计.jpg)

![1](image\mysql\逻辑模式设计.jpg)

### 设计原则

![1](image\mysql\第一范式.jpg)

### 并发控制

![1](image\mysql\并发问题.jpg)

![1](image\mysql\封锁.jpg)

![1](image\mysql\死锁.jpg)

## 数据库优化

### SQL及索引优化

#### 发现有问题的SQL

##### 1.检查慢查日志是否开启

![1](image\mysql\slow_query_log.jpg)

##### 2.查看所有日志的变量信息

![1](image\mysql\log.jpg)

![1](image\mysql\tail.jpg)

![1](image\mysql\日志.jpg)

##### 3.MySQL慢查日志的存储格式

![1](image\mysql\存储格式.jpg)

#### 慢查日志分析工具

##### 1.mysqldumpslow

![1](image\mysql\mysqldumpslow.jpg)

![1](image\mysql\10.jpg)

##### 2.pt-query-digest

![1](image\mysql\wget.jpg)

![1](image\mysql\summary.jpg)

![1](image\mysql\pt.jpg)

![1](image\mysql\diff.jpg)

![1](image\mysql\pt_find.jpg)

![1](image\mysql\pt_kill.jpg)

![1](image\mysql\grants.jpg)

![1](image\mysql\附录.jpg)

#### 如何通过慢日志查找有问题的SQL

![1](image\mysql\1和2.jpg)

![1](image\mysql\3.jpg)

#### 通过explain查询分析SQL的执行计划

![1](image\mysql\explain.jpg)

![1](image\mysql\含义.jpg)

![1](image\mysql\字段说明.jpg)

![1](image\mysql\字段2.jpg)

![1](image\mysql\字段3.jpg)

![1](image\mysql\字段4.jpg)

![1](image\mysql\字段5.jpg)

![1](image\mysql\字段6.jpg)

#### 具体慢查询优化案例

##### 1.函数Max()的优化

##### 2.函数Count()的优化

##### 3.子查询的优化

##### 4.group by的优化

##### 5.Limit查询的优化

##### 6.索引的优化



### 数据库结构的优化

#### 1.数据类型的选择

#### 2.数据库表的范式化优化

#### 3.数据库表的垂直拆分

![1](image\mysql\垂直.jpg)

![1](image\mysql\film.jpg)

![1](image\mysql\film_ext.jpg)

#### 4.数据库表的水平拆分

![1](image\mysql\水平.jpg)

![1](image\mysql\不拆分.jpg)

### 数据库系统配置优化



## 分库分表

## （一）数据库如何拆分

### 1.为什么要分库分表

①分库分表说白了，就是因为数据量太大了，如果你的单表数据量都到了千万级别，那么你的数据库就无法承受高并发的要求，数据库操作性能就会出现极大的下降。      

②数据库并发量太大了，一般而言，一个数据库最多支撑并发到2000，这时候一定要进行扩容，不然性能会出现严重下降。而且一个健康的单库并发值你最好保持在每秒1000左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。

### 2.有哪些分库分布中间件

比较常见的中间件有cobar、TDDL、atlas、sharding-jdbc、mycat。     

①**cobar** ：阿里b2b团队开发和开源的，属于proxy层方案。已经好几年没有进行更新了，基本没啥人用。而且不支持读写分离、存储过程、跨库join和分页等操作。     

②**TDDL** :淘宝团队开发的，属于client层方案，不支持join，但是支持读写分离。目前使用的也不多，因为还依赖淘宝的diamond配置管理系统。    

③**atlas** ：360开源的，属于proxy层方案，以前有一些公司在用，但是已经好几年没有更新了，所以现在用的也不多。   

④**sharding-jdbc** ：当当开源的，属于client层方案。SQL语法支持多，没有太多的限制，从2.0版本开始支持分库分表、读写分离、分布式id生成、柔性事务(最大努力送达型事务、TCC事务)。而且现在使用较多。     

⑤**myCat** ：基于cobar改造，属于proxy层方案，支持的功能完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。

### 3.分布式中间件类型

①**proxy类型**    
proxy类型的中间件就是一个客户端，需要直接部署一个中间件，去进行分库分表。服务端将sql发送到中间件客户端去进行不同表库的操作。如果中间件客户端不可用将直接导致无法进行分库分表，而且要走网络耗时。     

②**client**     
client不需要单独部署中间件客户端，运维成本低，中间件就是一个jar包，直接在项目中导入、配置就可以完成分库分表，而且不需要代理层的二次转发，性能高点，但是遇到升级等操作需要重新发布版本，各个系统都需要耦合sharding-jbdc的依赖。

### 4.垂直拆分与水平拆分

①**垂直拆分**           
垂直拆分的意思，就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，会将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。     
这个其实挺常见的，不一定我说，大家很多同学可能自己都做过，把一个大表拆开，订单表、订单支付表、订单商品表。    

②**水平拆分**     
水平拆分的意思，就是把一个表的数据给弄到多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的库里，然后用多个库来抗更高的并发，还有就是用多个库的存储容量来进行扩容。     

③**表的拆分**     
还有表层面的拆分，就是分表，将一个表变成N个表，就是让每个表的数据量控制在一定范围内，保证SQL的性能。否则单表数据量越大，SQL性能就越差。一般是200万行左右，不要太多，但是也得看具体你怎么操作，也可能是500万，或者是100万。你的SQL越复杂，就最好让单表行数越少。     

一般来说，垂直拆分，你可以在表层面来做，对一些字段特别多的表做一下拆分；水平拆分，是并发承载不了，或者是数据量太大，容量承载不了，你给拆了，按什么字段来拆，你自己想好；分表，你考虑一下，你如果哪怕是拆到每个库里去，并发和容量都ok了，但是每个库的表还是太大了，那么你就分表，将这个表分开，保证每个表的数据量并不是很大。    

### 5.两种分库分表方式

①**range方式**    
就是每个库一段连续的数据，这个一般是按比如时间范围来的，但是这种一般较少用，因为很容易产生热点问题，大量的流量都打在最新的数据上了。      

range来分，好处在于说，后面扩容的时候，就很容易，因为你只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写新的库了；缺点，但是大部分的请求，都是访问最新的数据。实际生产用range，要看场景，你的用户不是仅仅访问最新的数据，而是均匀的访问现在的数据以及历史的数据

②**hash方式**    
按照某个字段hash一下均匀分散，这个较为常用。

hash分法，好处在于说，可以平均分配没给库的数据量和请求压力；坏处在于说扩容起来比较麻烦，会有一个数据迁移的这么一个过程

### （二）如何进行分库分表的数据迁移

### 1.停机迁移方案

这是最简单的也是最low的迁移方案了，如果系统就算短期停机也没有关系或者造不成多大的影响，可以选用此方案。      

首先停掉机器，将系统全都停掉，不要再有新的数据进来，然后使用之前写好的程序，连接旧的数据库，将旧数据库里面的数据读出来，然后通过数据分发中间件写到分库分好的数据里面去。然后修改系统是数据库连接、分库分表配置，然后重新上线。     

### 2.双写不停机迁移方案

双写迁移方案的核心在双写，首先要修改系统所有需要写库的地方，将虽有对数据的写操作不但要写入就库，也要同时写入新库。    

然后使用写好的数据迁移程序，去读取老数据库的数据写入到新的数据库里面去，写的时候要根据数据的最后更新时间去判断数据，如果读出来的数据新库没有直接写入，如果新库也有，查看最后更新时间，旧库的新就覆盖写入，如果新库的新就放弃这条数据。     

导完一轮数据之后，有可能数据还是存在不一致，那么就写个程序做一轮校验，对比老库和新库的每条数据，如果存在不一样的，就针对这些不一样的，再次去进行数据同步。反复循环，直到数据完全一致。      

接着当数据完全一致了，就ok了，基于仅仅使用分库分表的最新代码，重新部署一次，不就仅仅基于分库分表在操作了么，还没有几个小时的停机时间，很稳。所以现在基本玩儿数据迁移之类的，都是这么干了。

### （三）动态扩容缩容的分库分表方案

### 1.扩容与缩容

这个是你必须面对的一个事儿，就是你已经弄好分库分表方案了，然后一堆库和表都建好了，基于分库分表中间件的代码开发啥的都好了，测试都ok了，数据能均匀分布到各个库和各个表里去，而且接着你还通过双写的方案咔嚓一下上了系统，已经直接基于分库分表方案在搞了。

那么现在问题来了，你现在这些库和表又支撑不住了，要继续扩容咋办？这个可能就是说你的每个库的容量又快满了，或者是你的表数据量又太大了，也可能是你每个库的写并发太高了，你得继续扩容。    

缩容就是现在业务不景气了，数据量减少，并发量下降，那么不能让他占着太多的数据库啊，肯定要进行缩容。    

### 2.停机扩容

①**原理**    
这个方案就跟停机迁移一样，步骤几乎一致，唯一的一点就是那个导数的工具，是把现有库表的数据抽出来慢慢倒入到新的库和表里去。但是最好别这么玩儿，有点不太靠谱，因为既然分库分表就说明数据量实在是太大了，可能多达几亿条，甚至几十亿，你这么玩儿，可能会出问题。

从单库单表迁移到分库分表的时候，数据量并不是很大，单表最大也就两三千万。写个工具，多弄几台机器并行跑，1小时数据就导完了

如果经过分库分表之后，那么数据量肯定会非常大，那么这种方案的耗时会太长。     

### 3.优雅设计扩容缩容

优雅的设计扩容缩容的意思就是 进行扩容缩容的代价要小，迁移数据要快。     

可以采用逻辑分库分表的方式来代替物理分库分表的方式，要扩容缩容时，只需要将逻辑上的数据库、表改为物理上的数据库、表。    

第一次进行分库分表时就多分几个库，一个实践是利用32 * 32来分库分表，即分为32个库，每个库32张表，一共就是1024张表，根据某个id先根据先根据数据库数量32取模路由到库，再根据一个库的表数量32取模路由到表里面。     

刚开始的时候，这个库可能就是逻辑库，建在一个mysql服务上面，比如一个mysql服务器建了16个数据库。   

如果后面要进行拆分，就是不断的在库和mysql实例之间迁移就行了。将mysql服务器的库搬到另外的一个服务器上面去，比如每个服务器创建8个库，这样就由两台mysql服务器变成了4台mysql服务器。我们系统只需要配置一下新增的两台服务器即可。     

比如说最多可以扩展到32个数据库服务器，每个数据库服务器是一个库。如果还是不够？最多可以扩展到1024个数据库服务器，每个数据库服务器上面一个库一个表。因为最多是1024个表么。

这么搞，是不用自己写代码做数据迁移的，都交给dba来搞好了，但是dba确实是需要做一些库表迁移的工作，但是总比你自己写代码，抽数据导数据来的效率高得多了。

哪怕是要减少库的数量，也很简单，其实说白了就是按倍数缩容就可以了，然后修改一下路由规则。      

②**分库分表路由规则**     
对库可以进行根据库的数量取模决定库的路由。     
对表可以先根据表的数量进行整除，然后将结果再对表的数量进行取模，这样可以跟均匀的分布。    

③**过程**    
1、设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是32库 * 32表，对于大部分公司来说，可能几年都够了

2、路由的规则，orderId 模 32 = 库，orderId / 32 模 32 = 表

3、扩容的时候，申请增加更多的数据库服务器，装好mysql，倍数扩容，4台服务器，扩到8台服务器，16台服务器

4、由dba负责将原先数据库服务器的库，迁移到新的数据库服务器上去，很多工具，库迁移，比较便捷

5、我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址

6、重新发布系统，上线，原先的路由规则变都不用变，直接可以基于2倍的数据库服务器的资源，继续进行线上系统的提供服务



### （四）分库分表的id主键生成

### 1.问题

其实这是分库分表之后你必然要面对的一个问题，就是id咋生成？因为要是分成多个表之后，每个表都是从1开始累加，那肯定不对啊，需要一个全局唯一的id来支持。所以这都是你实际生产环境中必须考虑的问题。 

### 2.生成方案

#### (1)数据库自增id方案

**原理** ：这个就是说你的系统里每次得到一个id，都是往一个库的一个表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个id。拿到这个id之后再往对应的分库分表里去写入。    
**优点** ： 操作简单。      
**缺点** ：单库生成自增id，如果是高并发，就会有瓶颈。    
**适合的场景**：你分库分表就俩原因，要不就是单库并发太高，要不就是单库数据量太大；除非是你并发不高，但是数据量太大导致的分库分表扩容，你可以用这个方案，因为可能每秒最高并发最多就几百，那么就走单独的一个库和表生成自增主键即可。    
**改进** ：可以给每个表指定不一样的自增主键起始值，然后按照表的数据去自增，比如有三个表，分别指定自增主键起始值为1、2、3，然后按照一次增加3的方式取增长，这样数据库1的id就是1、4、7、10.....,数据库2的id就是2、5、8、11......,数据库3的id就是3、6、9、12，这样就可以不依赖单点数据库。      

#### (2)UUID方案

 **原理** ：使用UUID去生成，保证唯一性。    
**优点** ：本地生成，不基于数据库，速度快不存在并发瓶颈。     
**缺点** ：UUID太长，作为主键性能太差了。    
**适用场景** ：如果你是要随机生成个什么文件名了，编号之类的，你可以用uuid，但是作为主键是不能用uuid的。   

#### (3)使用当前时间方案

**原理**  ：获取当前时间的时间戳来作为id。    
**优点** ：简单。     
**缺点** ：高并发的时候，会存在重复的情况。    
**适用场景** ：这个级别就不适用，不考虑。    
**改进**  ：一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个id，如果业务上你觉得可以接受，那么也是可以的。你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号，订单编号，时间戳 + 用户id + 业务含义编码。     

#### (4)snowflake算法

Twitter开源的分布式id生成算法，就是把一个64位的long型的id，第一个bit是不使用的，保证获得是一个正数；然后用后面的41个bit作为当前的毫秒数；用其后的10bit作为工作机器的id；最后的12个bit作为序列化。

![image.png](/image/mysql/4-1snowflask.png)

- 1 bit：不用，为啥呢？因为二进制里第一个bit为如果是1，那么都是负数，但是我们生成的id都是正数，所以第一个bit统一都是0
- 41 bit：表示的是时间戳，单位是毫秒。41 bit可以表示的数字多达2^41 - 1，也就是可以标识2 ^ 41 - 1个毫秒值，换算成年就是表示69年的时间。
- 10 bit：记录工作机器id，代表的是这个服务最多可以部署在2^10台机器上哪，也就是1024台机器。但是10 bit里5个bit代表机房id，5个bit代表机器id。意思就是最多代表2 ^ 5个机房（32个机房），每个机房里可以代表2 ^ 5个机器（32台机器）。
- 12 bit：这个是用来记录同一个毫秒内产生的不同id，12 bit可以代表的最大正整数是2 ^ 12 - 1 = 4096，也就是说可以用这个12bit代表的数字来区分同一个毫秒内的4096个不同的id

64位的long型的id，64位的long -> 二进制

0 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000000

2018-01-01 10:00:00 -> 做了一些计算，再换算成一个二进制，41bit来放 -> 0001100 10100010 10111110 10001001 01011100 00

机房id，17 -> 换算成一个二进制 -> 10001

机器id，25 -> 换算成一个二进制 -> 11001

snowflake算法服务，会判断一下，当前这个请求是否是，机房17的机器25，在2175/11/7 12:12:14时间点发送过来的第一个请求，如果是第一个请求，那么序列号就是000000000000

假设，在2175/11/7 12:12:14时间里，机房17的机器25，发送了第二条消息，snowflake算法服务，会发现说机房17的机器25，在2175/11/7 12:12:14时间里，在这一毫秒，之前已经生成过一个id了，此时如果你同一个机房，同一个机器，在同一个毫秒内，再次要求生成一个id，此时我只能把加1

0 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000001

比如我们来观察上面的那个，就是一个典型的二进制的64位的id，换算成10进制就是910499571847892992。    

大概这个意思就是说41 bit，就是当前毫秒单位的一个时间戳，就这意思；然后5 bit是你传递进来的一个机房id（但是最大只能是32以内），5 bit是你传递进来的机器id（但是最大只能是32以内），剩下的那个10 bit序列号，就是如果跟你上次生成id的时间还在一个毫秒内，那么会把顺序给你累加，最多在4096个序号以内。

所以你自己利用这个工具类，自己搞一个服务，然后对每个机房的每个机器都初始化这么一个东西，刚开始这个机房的这个机器的序号就是0。然后每次接收到一个请求，说这个机房的这个机器要生成一个id，你就找到对应的Worker，生成。

他这个算法生成的时候，会把当前毫秒放到41 bit中，然后5 bit是机房id，5 bit是机器id，接着就是判断上一次生成id的时间如果跟这次不一样，序号就自动从0开始；要是上次的时间跟现在还是在一个毫秒内，他就把seq累加1，就是自动生成一个毫秒的不同的序号。

这个算法那，可以确保说每个机房每个机器每一毫秒，最多生成4096个不重复的id。

利用这个snowflake算法，你可以开发自己公司的服务，甚至对于机房id和机器id，反正给你预留了5 bit + 5 bit，你换成别的有业务含义的东西也可以的。

这个snowflake算法相对来说还是比较靠谱的，所以你要真是搞分布式id生成，如果是高并发啥的，那么用这个应该性能比较好，一般每秒几万并发的场景，也足够你用了。

```java
public class IdWorker{

    private long workerId;
    private long datacenterId;
    private long sequence;

    public IdWorker(long workerId, long datacenterId, long sequence){
        // sanity check for workerId
// 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0
        if (workerId > maxWorkerId || workerId < 0) {
            throw new IllegalArgumentException(String.format("worker Id can't be greater than %d or less than 0",maxWorkerId));
        }
        if (datacenterId > maxDatacenterId || datacenterId < 0) {
            throw new IllegalArgumentException(String.format("datacenter Id can't be greater than %d or less than 0",maxDatacenterId));
        }
        System.out.printf("worker starting. timestamp left shift %d, datacenter id bits %d, worker id bits %d, sequence bits %d, workerid %d",
                timestampLeftShift, datacenterIdBits, workerIdBits, sequenceBits, workerId);

        this.workerId = workerId;
        this.datacenterId = datacenterId;
        this.sequence = sequence;
    }

    private long twepoch = 1288834974657L;

    private long workerIdBits = 5L;
    private long datacenterIdBits = 5L;
    private long maxWorkerId = -1L ^ (-1L << workerIdBits); // 这个是二进制运算，就是5 bit最多只能有31个数字，也就是说机器id最多只能是32以内
    private long maxDatacenterId = -1L ^ (-1L << datacenterIdBits); // 这个是一个意思，就是5 bit最多只能有31个数字，机房id最多只能是32以内
    private long sequenceBits = 12L;

    private long workerIdShift = sequenceBits;
    private long datacenterIdShift = sequenceBits + workerIdBits;
    private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits;
    private long sequenceMask = -1L ^ (-1L << sequenceBits);

    private long lastTimestamp = -1L;

    public long getWorkerId(){
        return workerId;
    }

    public long getDatacenterId(){
        return datacenterId;
    }

    public long getTimestamp(){
        return System.currentTimeMillis();
    }

    public synchronized long nextId() {
// 这儿就是获取当前时间戳，单位是毫秒
        long timestamp = timeGen();

        if (timestamp < lastTimestamp) {
            System.err.printf("clock is moving backwards.  Rejecting requests until %d.", lastTimestamp);
            throw new RuntimeException(String.format("Clock moved backwards.  Refusing to generate id for %d milliseconds",
                    lastTimestamp - timestamp));
        }

// 0
// 在同一个毫秒内，又发送了一个请求生成一个id，0 -> 1

        if (lastTimestamp == timestamp) {
            sequence = (sequence + 1) & sequenceMask; // 这个意思是说一个毫秒内最多只能有4096个数字，无论你传递多少进来，这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围
            if (sequence == 0) {
                timestamp = tilNextMillis(lastTimestamp);
            }
        } else {
            sequence = 0;
        }

// 这儿记录一下最近一次生成id的时间戳，单位是毫秒
        lastTimestamp = timestamp;

// 这儿就是将时间戳左移，放到41 bit那儿；将机房id左移放到5 bit那儿；将机器id左移放到5 bit那儿；将序号放最后10 bit；最后拼接起来成一个64 bit的二进制数字，转换成10进制就是个long型
        return ((timestamp - twepoch) << timestampLeftShift) |
                (datacenterId << datacenterIdShift) |
                (workerId << workerIdShift) |
                sequence;
    }


    private long tilNextMillis(long lastTimestamp) {
        long timestamp = timeGen();
        while (timestamp <= lastTimestamp) {
            timestamp = timeGen();
        }
        return timestamp;
    }

    private long timeGen(){
        return System.currentTimeMillis();
    }

    //---------------测试---------------
    public static void main(String[] args) {
        IdWorker worker = new IdWorker(1,1,1);
        for (int i = 0; i < 30; i++) {
            System.out.println(worker.nextId());
        }
    }

}
```

### （五）MYSQL读写分离

### 1.为什么进行读写分离

这个，高并发这个阶段，那肯定是需要做读写分离的，啥意思？因为实际上大部分的互联网公司，一些网站，或者是app，其实都是读多写少。所以针对这个情况，就是写一个主库，但是主库挂多个从库，然后从多个从库来读，那不就可以支撑更高的读并发压力了吗？     

### 2.如何实现mysql读写分离

其实很简单，就是基于主从复制架构，简单来说，就是搞一个主库，挂多个从库，然后我们就只是写主库，然后主库自动把数据同步到从库，读数据全都走从库。     

### 3.MYSQL主从复制原理

主库将数据操作指令写到binlog日志，然后从库连接主库之后，从库有一个IO线程，将主库的binlog日志拷贝到自己本地，写入一个中继日志中。接着从库中有一个SQL线程会从中继日志中读取binlog，然后执行binlog日志中的内容，也就是在自己本地再执行一遍SQL，这样就可以保证自己跟主库的数据是一致的。      

![image.png](/image/mysql/5-1主从复制原理.png)
​    
这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所有在高并发场景下，从库的数据一定会别主库慢一些，，是有延时的。所以经常出现，刚写入的数据可能是读不到的问题，要经过几十毫秒，甚至几百毫秒才能读取到。     

还有一个问题，就是如果一个数据已写入主库，但是这时候binlog从库还没有来得及复制。主库这时候宕机了，那么有些数据可能在从库上是没有的，有些数据就会丢失。    

### 4.解决主库数据丢失问题

mysql有一个机制叫做半同步复制，这个机制就可以解决主库数据丢失问题。    
这个所谓半同步复制，semi-sync复制，指的是主库写入binlog日志之后，就会将数据立即强制将数据同步到从库。从库将日志写入自己本地的relay log之后，接着会返回一个ack给主库，主库接收到至少一个从库的ack之后才会认为写操作完成了。   

### 5.解决主从同步延迟

mysql可以在复制binlog文件时并行复制，减轻延迟问题。   
这个所谓并行复制，指的是从库开启多个线程，并行读取relay log中**不同库** 的日志，然后并行重放不同库的日志，这是库级别的并行。  

### 6.mysql主从延时问题

show status，Seconds_Behind_Master，你可以看到从库复制主库的数据落后了几ms。 

其实这块东西我们经常会碰到，就比如说用了mysql主从架构之后，可能会发现，刚写入库的数据结果没查到，结果就完蛋了。。。。

所以实际上你要考虑好应该在什么场景下来用这个mysql主从同步，建议是一般在**读远远多于写** ，而且读的时候一般对数据时效性要求没那么高的时候，用mysql主从同步。

所以这个时候，我们可以考虑的一个事情就是，你可以用mysql的并行复制，但是问题是那是库级别的并行，所以有时候作用不是很大

所以这个时候通常来说，我们会对于那种写了之后立马就要保证可以查到的场景，采用强制读主库的方式，这样就可以保证你肯定的可以读到数据了吧。其实用一些数据库中间件是没问题的。

一般来说，如果主从延迟较为严重

1、分库，将一个主库拆分为4个主库，每个主库的写并发就500/s，此时主从延迟可以忽略不计。 

2、打开mysql支持的并行复制，多个库并行复制，如果说某个库的写入并发就是特别高，单库写并发达到了2000/s，并行复制还是没意义。28法则，很多时候比如说，就是少数的几个订单表，写入了2000/s，其他几十个表10/s。

3、重写代码，写代码的同学，要慎重，不要做插入之后立即查询操作，插入数据之后，直接就更新，不要查询。

4、如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你这么搞导致读写分离的意义就丧失了



### （六）索引

### 零.索引简介

#### 1. 索引是什么

①MySQL官方对索引的定义是：索引(Index)是帮助MySQL高效获取数据的数据结构。       
②可以简单的理解为“排好序的快速查找数据结构”。     
③除了数据本身之外，数据库还维护着一个满足特定查找算法的数据结构，这种数据结构以某种方式指向数据，这样就可以在这些数据结构的基础上实现高级查找算法，这种数据结构就是索引。     
④一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储在磁盘上。     
⑤我们平常所说的索引，如果没有特别说明，都是值B+树结构组织的索引。其中聚集索引，稀疏索引、覆盖索引、复合索引、前缀索引、唯一索引，默认都使用B+树索引，统称索引。当然除了B+树之外还有哈希索引、BitMap索引等。      

#### 2.索引的优势

①类似图书的目录索引，提高了数据检索的效率，降低数据库的IO成本。     
②通过索引对数据进行排序，降低数据排序的成本，降低了CPU的消耗。     

#### 3.索引的劣势

①实际上索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录，所以索引也是要占用空间的。      
②虽然索引大大提高了查询速度，同时却会降低更新表的数据，如对表进行INSERT、UPDATE、DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件；每次更新添加了索引列的字段，都会调整因为更新所带来的键值变化后的索引信息。     
③索引只是提高效率的一个因素，如果你的MySQL有大数据量的表，就需要花费时间研究建立最优的索引。或者优化查询语句。   

#### 4.索引的分类

①单值索引：一个索引包含单个列，一个表可以有多个单列索引。       
②唯一索引：索引列的值必须唯一，但允许有空值。       
③复合索引：一个索引包含多个列。       

### 一.使用二叉查找树作为索引存在的问题

> 二叉查找树（Binary Search Tree），（又：[二叉搜索树](https://baike.baidu.com/item/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/7077855)，二叉排序树）它或者是一棵空树，或者是具有下列性质的[二叉树](https://baike.baidu.com/item/%E4%BA%8C%E5%8F%89%E6%A0%91/1602879)： 若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为[二叉排序树](https://baike.baidu.com/item/%E4%BA%8C%E5%8F%89%E6%8E%92%E5%BA%8F%E6%A0%91/10905079)。

![使用二叉搜索树作为索引.png](/image/mysql/6-1.png)

如上面的图所示，它存在的问题如下：     
①如果数据太多而且大量数据单调，那么二叉搜索树就会在一条分支上一直进行延伸，这样基本就由二叉搜索树变成了线性表，搜索效率就会大大下降。      
②一个节点只能存储一个值，数据量太大的时候就会一颗高度特别高的二叉树，这样搜索效率就会出现大幅下降。      
③使用平衡二叉搜索树，虽然可以防止编程线性表，但是每次更新索引都会树的旋转，这样就会消耗特别大的性能，而且一个节点还是只能保存一个值，无法解决树过高的问题。 

### 二.使用B树作为索引存在的问题

> 一棵m阶B树(balanced tree of order m)是一棵平衡的m路搜索树。它或者是空树，或者是满足下列性质的树：
> 1、根结点至少有两个子女；
> 2、每个非根节点所包含的关键字个数 j 满足：┌m/2┐ - 1 <= j <= m - 1；
> 3、除根结点以外的所有结点（不包括叶子结点）的度数正好是关键字总数加1，故内部子树个数 k 满足：┌m/2┐ <= k <= m ；
> 4、所有的叶子结点都位于同一层。
> 在B-树中，每个结点中关键字从小到大排列，并且当该结点的孩子是非叶子结点时，该k-1个关键字正好是k个孩子包含的关键字的值域的分划。      

![使用B树作为索引.png](/image/mysql/6-2.png)

①相比二叉树，他一个节点可以容纳多个节点，可以降低树的高度，增加查找效率。       
②但是一层能容纳的数量还是太少，数据量一大树的高度也会变得很高，搜索效率一样会进行大幅度的下降。      
③一个节点能存放的数据要比指针少一，数据保存率太低

### 三.使用B+树作为索引

> **B+ 树**是一种树数据结构，通常用于[数据库](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%BA%93)和[操作系统](https://baike.baidu.com/item/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F)的[文件系统](https://baike.baidu.com/item/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F)中。B+ 树的特点是能够保持数据稳定有序，其插入与修改拥有较稳定的对数时间复杂度。B+ 树元素自底向上插入，这与[二叉树](https://baike.baidu.com/item/%E4%BA%8C%E5%8F%89%E6%A0%91)恰好相反。
> B+ 树在节点访问时间远远超过节点内部访问时间的时候，比可作为替代的实现有着实在的优势。这通常在多数节点在次级存储比如[硬盘](https://baike.baidu.com/item/%E7%A1%AC%E7%9B%98)中的时候出现。通过最大化在每个[内部节点](https://baike.baidu.com/item/%E5%86%85%E9%83%A8%E8%8A%82%E7%82%B9)内的子节点的数目减少树的高度，平衡操作不经常发生，而且效率增加了。这种价值得以确立通常需要每个节点在次级存储中占据完整的磁盘块或近似的大小。

![B+树作为索引.png](/image/mysql/6-3.png)

如图是一个B+树，这时候如果我们要查询46这个节点，那么只需要在第一节点中进行查找，发现46在45和67之间，然后如何就走第二个节点，走到了（45,48,63）这个节点，然后发现46在45-48之间，走第一个节点，如何遍历，就找到了46。       
①使用B+树，一个节点可以存储的值和它指向一下个节点的指针一样多，可以多存储数据。     
②所有父节点的数据同时也保存在子节点之中，这样所有数据都在一起，比较连续。    
③叶子节点的所有值由一个链表进行连接，这样如果进行范围查询就非常简单，只需顺着链表往下走就行。      

### 四.Hash和BitMap索引

#### 1.Hash索引

Hash索引是指，将要创建索引的字段值进行hash，按照hash值为key，数据地址为value，保存成为一个HashMap结构，如果一个hash值对应多个行数据，就进行链表存储。      

![使用Hash索引.png](/image/mysql/6-4.png)

①使用hash只需要进行一次hash就可以查找到值，不需要进行像二叉树一样的逐层查找，效率高。      
②由于数据是进行hash之后进行保存的，所以原数据在里面是没有顺序的，所以仅仅能满足 “=” 、“IN”这样的操作，不能使用范围查询 。       
③无法被用来避免数据的排序操作，因为不像B+树，它的存储是有顺序的，是按照索引值进行排序的，然后hash索引是hash后的数据，所以无法避免排序。      
④不能利用部分索引键查询，在B+树中，多个字段的索引是通过按照字段一次排序进行存储的，所以如果创建了(A,B,C)三个字段的联合索引，那么可以使用部分索引例如A，(A,B)，但是hash的联合索引是所有字段一起进行hash的结果去映射的，所以部分字段无法使用。       
⑤不能避免全表扫描。因为查询出来的数据还是存在buckets中的，还是要进行表的遍历。     
⑥遇到大量hash值相等的情况后，也就是出现大量的hash碰撞后，链表就会非常长，变成线性结构，查询效率并不一定会比B+-Tree高。       

#### 2.BitMap索引

BitMap索引使用不同的值进行分组，然后每组数据中将相同的值的位置标位1，不同的值的位置标为0，这样就会形成很多个不同的二进制串。     

![BitMap示意.png](/image/mysql/6-5.png)

①BitMap是对数据进行位图存储，所以查询会很快。      
②由于每个值都需要一个位图去保存，所以只适用于值的种类比较少的情况，比如性别这样的字段。      
③进行数据的统计速度快。     
④锁的力度大，因为进行增删改的时候，位图的值的位置会发生变化，所以会锁住整个位图的对象。     

### 五.密集索引和稀疏索引和覆盖索引

#### 1.密集索引

①在InnoDB引擎中，数据本身就是索引文件，其表数据文件本身就是B+树组织的一个索引结构，树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。这种索引被称为"聚集索引(或者聚族索引)"。      
②MyISAM引擎的索引不是聚集索引。     
③InnoDB有且只有一个聚集索引，选取规则如下：      
A：如果定义了主键，那么该主键就作为密集索引。     
B: 如果没有定义主键，则选择该表的第一个唯一非空索引作为密集索引。     
C：如果上面都不满足，InnoDB会在内部生成一个六字节的自增值作为隐藏主键，来作为密集索引。    
④密集索引文件中的每个搜索码值都对应一个索引值。
⑤密集索引决定了数据的物理存放顺序，一个文件只能有一个物理存放顺序，所以只能有一个密集索引。     
⑥密集索引的叶子节点之间存放的该行数据。

#### 2.稀疏索引

①MyISAM引擎的所有索引都是稀疏索引。     
②稀疏索引文件只为搜索码的某些值建立索引。   
③叶子节点只保留了数据的地址或者主键，获取到了之后还要跟地址或者主键去再次进行查找。    

#### 3.覆盖索引

如果一个索引包含(或者说覆盖)所有需要查询的字段的值，那么这个索引就是"覆盖索引"。     
①覆盖索引会把要查询出来的列和索引进行对应，查询时不会进行回表操作(不会进行二次查询)。     

### 六.InnoDB和MyISAM的区别

①数据的物理存放不同。     
InnoDB在文件存放中会存在两个文件，一个frm结尾的文件保存了这个表的定义文件，数据和索引都存放在一个以ibd结尾的文件中。     
MyISAM在文件存放中存在三个文件，一个frm结尾的文件保存了这个表的定义文件，索引存放在一个以MYI结尾的文件中，数据存放在一个以MYD结尾的文件中。   
②索引不同    
InnoDB存在一个密集索引，叶子节点保存的该列的所有值；MyISAM所有的索引都是稀疏索引，叶子节点保存的是数据的地址；InnoDB稀疏索引的叶子节点存放的是主键。     
③MyISAM缓存有表meta-data(行数等信息)，所以在进行count(*)的时候会很快，InnoDB没有。    
④MyISAM强调的是性能，每次查询具有原子性，其执行速度比InnoDB类型更快。    
⑤MyISAM不支持事务操作。InnoDB支持事务。    
⑥MyISAM不支持外键，二InnoDB支持。

### 七.索引的创建条件

#### 1.需要创建索引的情况

①主键自动建立唯一索引。     
②频繁作为查询条件的字段应该创建索引。     
③查询中与其他表关联的字段，外键等应该建立索引。      
④频繁更新的字段不适合创建索引。       
⑤where条件里用不到的字段不创建索引。      
⑥在单值索引和复合索引之间，优先选择复合索引。    
⑦查询中排序的字段，应该建立索引。     
⑧查询中进行统计或者分组的字段应该建立索引。     

#### 2.不需创建索引的情况

①表记录太少的情况。    
②经常进行增删改的表。     
③数据重复且分布均匀的表字段。        

### 八.explain指令

#### 1.能干什么

使用explain关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的，分析你的查询语句或是表结构的性能瓶颈。      
通过explain关键字可以查看如下的一些信息。    
①表的读取顺序。    
②数据读取操作的操作类型。     
③那些索引可以使用。     
④那些索引实际被使用。      
⑤表之间的引用关系。     
⑥每张表有多少行被执行器查询。    

#### 2.怎么用

在SQL语句前面加入explain 关键字，执行该语句就可以获取信息。     

![explain执行结果.png](/image/mysql/6-6.png)

①id       
select查询的序号列，包含一组数字，表示查询中执行select子句或操作表的顺序。      
A : id相同，表示顺序由上而下执行。     
B : id不同，如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行。    
C : id相同不同，同时存在。     

②select_type      
查询的类型，主要用于区别 普通查询、联合查询、子查询等的复杂查询。有下面几种值：      
A ：SIMPLE 简单的select语句，查询中不包含子查询或者union。     
B : PRIMARY 查询中若包含任何复杂的子查询，最外层查询则被标记为PRIMARY.      
C : SUBQUERY 在select或者where列表中包含子查询，会被标记为SUBQUERY。     
D : DERIVED 在from列表中包含的子查询被标记为DERIVED(衍生),mysql会递归执行这些子查询，把结果放在临时表里。      
E : UNION 若第二个select出现在union之后，则被标记为union，若union包含在from子句的子查询中，外层select被标记为DERIVED。      
F : UNION RESULT  从UNION表获取结果的select。       

③table     
显示这一行的数据是关于那张表的。     

④type      
显示查询使用了何种类型，从走好到最差依次是下面顺序。     
A : **system** :表示只有一行记录，这是const类型的特例，平时不会出现，这个可以忽略不计。      
B : **const** : 表示通过索引一次就找到了，const用于表示primary key或者unique索引。因为只匹配一行数据，所以很快。     
C : **eq_ref** :唯一性索引扫描，对应每个索引键，表中只有一条记录与之匹配。常见于主键或唯一索引扫描。      
D : **ref** :非唯一索引扫描，返回匹配某个单独值的所有行；本质上也是一种索引访问，它返回所有匹配某个单独值的行，然而，它可能会找到很多个符合条件的行，所以他应该属于查找和扫描的混合体。     
E : **range** :只检索给定范围的值，使用一个索引选择行。key列显示使用了那个索引，一般就是在where语句中出现了between、<、>、in等的查询。这种范围扫描索引比全表扫描要好，因为它只需要开始于索引的某一点，而结束于另一点，不用扫描全部索引。     
F : **index** :Full Index Scan,index与All的区别为index类型只遍历索引树。这通常比All快，因为索引文件通常比数据文件少。也就是说虽然All和Index都是读全表，但index是从索引里面读，而all是从硬盘里面读。     
G : **all** :Full Table Scan,将遍历全表找到匹配的行。      
一般来说，得保证查询至少达到range级别，最好达到ref级别。     

⑤possible_keys     
显示可能在这张表表中的索引，一个或多个。查询涉及到的字段上若存在索引，则该索引被列出，但不一定被查询实际使用.       

⑥key      
实际使用到的索引，如果为null，则表示没有使用索引。查询中若使用覆盖索引，则索引和查询的select字段重叠。      

⑦key_len     
表示索引中使用的字节数，可以通过该列计算查询中使用的索引的长度，在不损失精确性的情况下，长度越短越好。 key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len使根据表定义计算而得的，不是通过表内检索出来的。     

⑧ref      
显示索引的哪一列被使用了，如果可能的话，是一个常数，那些列或常量被用于查询索引上面的值。      

⑨rows    
根据表统计信息及索引选用情况，大致估算出找到所需记录需要读取的行数。    

⑩Extra      
包含不合适在其他列中显示但十分重要的额外信息。有下面的一些取值：      
①**Using filesort** ： 说明mysql 会对数据使用一个外部排序，而不是按照表内的索引顺序就行排序。MYSQL中无法利用索引完成的排序操作成为"文件排序"。     

②**Using temporary** :使用了临时表保存中间结果，mysql在对查询结果排序时使用临时表。常见于排序order by和分组查询group by.。     

③**Using index** : 表示相应的select操作中使用了覆盖索引，避免访问了表的数据行，效率不错；如果同时出现using where ，表明索引被用来执行索引键值的查找；如果没有同时出现using where，表明索引用来读取数据而非执行查找动作。      

④**Using where** : 表明使用了where过滤。     

⑤**Using join buffer** :表明使用了连接缓存。    

⑥**impossible where** ：where子句的值总是false，不能用来获取任何元组。     

⑦**select tables optimized away** :在没有group by子句的情况下，基于索引优化MIN/MAX操作或者对于MyISAM存储引擎优化count(*)操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化。     

⑧**distinct** :优化distinct操作，在找到第一匹配的元组后即停止找同样值的动作。



### 九.索引失效

①复合索引全值匹配最好。      
②最左前缀匹配原则。      
③不在索引列上做任何操作(计算、函数、类型转换)，会导致索引失效而转向全表扫描。     
④存储引擎不能使用索引范围中范围条件右边的列(比如索引(A,B,C),如果查询条件为A=xxx and B > xx and C = xxx,则只会使用索引A，B使用了范围查询，不会使用C)。      
⑤尽量使用覆盖索引，减少select *。       
⑥mysql 在使用不等于(!= 或者 <>)的时候无法使用索引会导致全表扫描。     
⑦is null、is not null也无法使用索引。    
⑧like以通配符开头('%abc')mysql会使用全表扫描。     
⑨字符串不加单引号索引失效。     
⑩少用or，用它来连接会导致索引失效。      
**建议**      
①对于单值索引，尽量选择针对当前query过滤性更好的索引。      
②在选择组合索引的时候，当前query中过滤性最好的字段在索引字段顺序中，位置越靠前越好。      
③在选择组合索引的时候，尽量选择可以能够包含当前query中的where子句中更多字段的索引。     
④尽可能通过分析统计信息和调整query的写法来达到选择合适索引的目的。

### 十.最左前缀匹配原则

①mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如 a=3 and b=4 and c>5 and d=6,如果建立(a,b,c,d)顺序的索引，d就用不到索引；如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序是可以随意调整的。这就是最左匹配原则。     
②= 和 in 是可以乱序的，比如 a=1 and b=2 and c=3,建立(a,b,c)索引可以任意顺序，mysql查询优化器水帮助我们调整顺序。    
③为什么？      
因为mysql对于联合索引的处理是，首先根据第一个字段进行排序，然后在排序的基础上再根据第二个字段进行排序，类似于group by 对多个字段进行排序，所以对于第一个字段(最左)来说是绝对有序的，但是对于后面的字段来说就是无序的了。这就是最左匹配原则的成因。

![示意图说明.png](/image/mysql/6-7.png)



### 十一.查询优化

#### 1.用于小表驱动大表。

①驱动表的定义

当进行多表连接查询时， [驱动表] 的定义为：      
1）指定了联接条件时，满足查询条件的记录行数少的表为[驱动表]。      
2）未指定联接条件时，行数少的表为[驱动表]（Important!）。     

②为什么？    
可以使用嵌套循环来进行类比    

```
for(int i=5;.......)
{
     for(int j=1000;......)
     {}
}
```

如果小的循环在外层，对于数据库连接来说就只连接5次，进行5000次操作，如果1000在外，则需要进行1000次数据库连接，从而浪费资源，增加消耗。这就是为什么要小表驱动大表。     

③怎么做    
下面结论都是针对in或exists的。     
in后面跟的是小表，exists后面跟的是大表。    
对于exists

select .....from table where exists(subquery);

可以理解为：将主查询的数据放入子查询中做条件验证，根据验证结果（true或false）来决定主查询的数据是否得以保留。

#### 2.order by关键字优化

①order by子句，尽量使用index方式排序，避免使用filesort排序。   
②尽可能在索引列上完成排序操作，遵照索引建的最佳左前缀原则。      
③如果不在索引列上，filesort有两种算法：mysql会启用双路排序和单路排序。     
A : 双路排序：mysql4.1之前使用双路排序，字面意思就是两次扫描磁盘，最终得到数据，读取
行指针和order by列，对他们进行排序，然后扫描已经排序好的列表，按照列表中的值重新从列表中读取对应的数据输出。简单来说就是：从磁盘取排序字段，然后在buffer中进行排序，再从磁盘取其它字段。        
B : 从磁盘中读取查询需要的所有列，按照order by列在buffer对它们进行排序，然后扫描排序后的列表进行输出，它的效率要快一些，避免了第二次读取数据，并且把随机IO变成了顺序IO，但它会使用更多的空间。    

④优化策略       

- 增大sort_offset_size 参数的设置。     
- 增大max_length_for_sort_data 参数的设置。     

#### 3.group by优化

①group by实质是先排序再进行分组，遵照索引建的最佳左前缀原则。     
②当无法使用索引列，增大max_length_for_sort_data参数的设置+增大sort_buffer_size参数的设置。     
③where高于having，能写在where限定的条件就不要去having限定了。        



### （七）事务

### 零.MyISAM和InnoDB关于锁的区别

①MyISAM默认用的是表级锁，不支持行级锁。

②InnoDB默认用的是行级锁，也支持表级锁。

③共享锁和排它锁的兼容性
|X|排它锁|共享锁|
-|-|-
排它锁|冲突|冲突
共享锁|冲突|兼容

④使用场景
**MyISAM**          
A: 频繁执行全部count语句。     
B: 对数据进行增删改的频率不高，查询非常频繁。     
C:不需要支持事务。     
**InnoDB**        
A:数据增删改查相当频繁。      
B: 要求支持事务。     

### 一.锁的分类

按所粒度分：    
表级锁：整个表加锁     
行级锁：对行数据加锁      
页级锁： 介入表级个页级之间的锁，锁定位于一个存储快的相邻的几行数据。

按锁级别分：          
共享锁：针对同一份数据，多个读操作可以同时进行而不会相互影响。      
排它锁： 当前写操作没有完成前，它会阻止其他写锁和读锁。

按加锁方式分  
自动锁：像意向锁、MyISAM的增删改查时加的锁就是自动锁，这是mysql自动加的锁。     
显式锁：像select for update，lock这种我们现实加的锁就是显式锁。   

按操作方式分               
DML锁：对数据进行操作时加的锁。        
DDL锁：对表结构进行变更加的锁。

按使用方式分          
乐观锁：认为数据不会造成冲突，在提交时才进行判断，不使用数据库的锁机制，而是使用版本号或者时间戳实现。  
悲观锁：对外界的影响处于保守状态，在处理中将数据锁定，往往依靠数据库提供的锁机制。全程使用排它锁锁定，先获取锁在执行。

### 三.数据库事务四大特性

1.原子性    
事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用。      
2.一致性       
执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的。     
3.隔离性      
并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的。     
4.持久性      
一个事务被提交之后，它对数据库中数据的改变是持久性的，即使数据库发生故障也不应该对其有任何影响。

### 三.并发事务带来的问题

1.脏读       
当一个事务正在访问数据并且对数据进行修改，而这种修改还没有提交到数据库中，这时另一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这时数据是"脏数据"，依据"脏数据"所做的操作可能是不正确的。      
2.丢失修改       
指在一个事务读取一个数据时，另外一个事务也访问了这个数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就会被丢失，称为丢失数据。      
3.不可重复读      
指在一个事务内多次读同一个数据。在这个事务还没有结束时，另一个事务也访问了该数据，那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，称为不可重复读。      
4.幻读       
幻读与不可重复读类似。它发生在一个事务读取了几行数据，接着另一个并发事务插入了一些数据时。在随后的的查询中，第一个事务就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。

### 四.数据库事务隔离机制

1.READ-UNCOMMITTED(读取未提交)         
最低的隔离级别，允许读取尚未提交的数据变更，**可能会导致脏读、幻读、不可重复读**。      
2.READ-COMMITTED(读取已提交)        
允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读和不可重复读任有可能发生**。     
3.REPEATABLE-READ(可重复读)        
对同一个字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复做，但幻读仍有可能发生**。       
4.SERIALIZABLE(可串行读)           
最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样的事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。     

| 隔离级别             | 脏读   | 不可重复读 | 幻读   |
| ---------------- | ---- | ----- | ---- |
| READ-UNCOMMITTED | √    | √     | √    |
| READ-COMMITTED   | ×    | √     | √    |
| REPEATABLE-READ  | ×    | ×     | √    |
| SERIALIZABLE     | ×    | ×     | ×    |

MySQL InnoDB存储引擎的默认支持的隔离级别是**REPEATABLE-READ(可重复读)** 。可以通过@@tx_isolation命令查看。       

这里需要注意的是：与SQL标准不同的地方在于InnoDB存储引擎在**REPEATABLE-READ(可重复读)** 事务隔离级别下使用的是Next-key Lock算法，因此可以避免幻读的产生。所以InnoDB存储引擎的默认支持的隔离级别是**REPEATABLE-READ(可重复读)** 已经可以完全保证事务的隔离性要求，即达到了SQL标准的**SERIALIZABLE(可串行化)**隔离级别。      

因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是READ-COMMITTED；但是InnoDB存储引擎默认使用**REPEATABLE-READ** 并不会有任何性能损失。     
InnoDB存储引擎在分布式事务的情况下一般会用到**SERIALIZABLE**隔离级别。

### 五.当前读与快照读

#### 1.当前读

select... lcok in share mode,select.....for update,update,delete,insert等操作都是当前读。

#### 2.快照读

不加锁的非阻塞读，select操作就是快照读，基于多版本操作。   
快照读的实现：     
1.依赖于数据行里的DB_TRX_ID、DB_ROLL_PTR、DB_ROW_ID字段。     
DB_TRX_ID:标志了最近一次操作这行数据的id。     
DB_ROLL_PTR：回滚指针，直写入回滚段的rollback segment的undo日志记录。    
DB_ROW_ID：表示行号，包含一个随着新行加入单调自增的记录(隐藏主键)。   
2.undo日志      
当我们对记录进行了变更操作时，就会产生undo日志，undo中记录的是老版数据，当一个旧的事务需要读取记录时，顺着undo链就可以读取到满足需要的老版本数据。分为insert undolog和update undolog，insert undolog只在事务回滚时被需要，并且在事务提交之后就可以被丢弃；update undolog会在对数据进行更新和删除时产生，不仅在事务混滚时需要，而且在进行快照读的时候也需要，因此不能随便删除。  

![image.png](/image/mysql/7-1.png)

3.read view      
主要用于做可见性判断，当我去执行快照读select的时候，会针对我们需要读的数据去创建一个read view，决定当前能够读取到的数据是哪个版本；主要基于将数据的DB_TRX_ID与当前活跃的事务的ID进行对比，如果等于就根据undolog去取上层的数据，知道取到比他小的数据。

### 六.RR如何避免幻读

1.表象：快照读(非阻塞读)--伪MVCC   
2.内在，next-key锁(行锁+gap锁)
**GAP锁**         
Gap锁锁定一个范围，防止出现幻读。     
1.对主键索引或者唯一锁会有Gap锁？     
如果where条件全部命中，则不会用Gap锁，只会加行锁。     
2.如果where条件全部不命中，则会用Gap锁。  
3.如果where条件部分命中，则会用Gap锁。   
4.Gap锁会用在非唯一索引或者不走索引的当前读中。   
非唯一索引情况：     

![非唯一索引.png](/image/mysql/7-2.png)

如上图，操作数据9，会锁住(6,9],(9,11]这两个区间即(6,11]的区间会被加上Gap锁，不被允许操作，这样就保证了防止幻读的发生。gap锁还要和主键值搭配才能精确判断，比如(6,11]这个区间被锁住，但是6对应的主键是c，如果插入(a,6)这样的数据，是可以插入的，但是(d,6)这样的数据就是无法插入的。    

不走索引：     
不走索引会加表锁，也就是加全部的Gap锁，

![image.png](/image/mysql/7-3.png)

### 七.锁的建议优化

1.尽可能让所有数据检索都用过索引来完成，避免无索引行锁升级为表锁。      
2.合理设计索引，尽量缩小锁的范围。       
3.尽可能减少检索条件，避免间隙锁(Gap锁)。       
4.尽量控制事务大小，减少锁定资源量和时间长度。     
5.尽可能使用低级别的事务隔离。      